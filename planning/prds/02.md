# Product Requirements Document (PRD)

## LLM-Based Condition Scoring System

**Version:** 1.0  
**Date:** 2025-12-28  
**Status:** Draft

---

## Overview

Replace the current heuristic-based condition scoring system with an LLM-based approach using Groq's fast inference API. This will provide more nuanced, context-aware scoring that considers sport-specific requirements, spot-specific characteristics, and temporal context.

---

## Goals

1. **Completely Replace Heuristics with LLM Scoring**: Remove all rule-based criteria matching, use only LLM evaluation
2. **Context-Aware Scoring**: Leverage time series data from database (12 hours before/after) for better condition assessment
3. **Sport & Spot-Specific Evaluation**: Use system prompts that understand both the sport requirements and spot-specific characteristics
4. **Database Integration**: Store scores (0-100 with reasoning) alongside forecast data for efficient querying and display
5. **Scalability**: Design for future personalization (V2) where users can define custom prompts

---

## Current State Analysis

### Existing Scoring System

**Location**: `lib/criteria.js`

**Current Approach**:

- Rule-based heuristics (`matchesWingfoilCriteria`, `matchesSurfingCriteria`)
- Binary matching (meets/doesn't meet criteria)
- Client-side evaluation during data display
- No scores stored in database

**Limitations**:

- Binary pass/fail doesn't capture quality gradations
- No consideration of temporal context (trends, improving/worsening conditions)
- No spot-specific nuance (same criteria applied to all spots)
- No reasoning or explanation for scores

### Data Flow

**Current Scraping Flow**:

```
scripts/scrape.mjs
  → lib/scraper.js (getForecast)
  → convex/spots.ts (saveForecastSlots mutation)
  → forecast_slots table (no scores)
```

**New Scraping Flow** (with async scoring):

```
scripts/scrape.mjs
  → lib/scraper.js (getForecast)
  → convex/spots.ts (saveForecastSlots mutation)
  → forecast_slots table (saved immediately, no scores yet)
  → Trigger scoreForecastSlots action (fire-and-forget, async)
  → scoreForecastSlots action runs separately:
    → For each slot: score and update via updateSlotScore mutation
```

**Current Display Flow**:

```
app/page.js
  → Fetch forecast_slots
  → lib/criteria.js (client-side matching - TO BE REMOVED)
  → lib/slots.js (enrichSlots - adds matchesCriteria, isIdeal flags - TO BE UPDATED)
  → Display filtered results
```

**Note**: Heuristic-based matching in `lib/criteria.js` and `lib/slots.js` will be completely removed and replaced with LLM score-based filtering.

---

## Requirements

### 1. LLM Scoring Integration

#### 1.1 LLM Provider: Groq

**Rationale**:

- Fast inference (low latency for real-time scoring)
- Cost-effective for high-volume scoring
- Good model selection (Llama 3.1, Mixtral, etc.)

**Model Selection**:

- **Model**: `openai/gpt-oss-120b` (Groq-hosted OpenAI OSS model)
- **Configuration**: Temperature 0.3 (consistent scoring), max_tokens 500

#### 1.2 Scoring Format

**Score Structure**:

```typescript
{
  score: number,        // 0-100 (integer)
  reasoning: string,    // Brief explanation (1-2 sentences)
  factors: {            // Breakdown of contributing factors
    windQuality?: number,      // 0-100 (for wingfoiling)
    waveQuality?: number,       // 0-100 (for surfing)
    tideQuality?: number,        // 0-100 (for surfing)
    overallConditions?: number   // 0-100 (general)
  }
}
```

**Score Interpretation**:

- **90-100**: Exceptional conditions (epic)
- **75-89**: Excellent conditions (ideal)
- **60-74**: Good conditions (matches criteria)
- **40-59**: Marginal conditions (usable but not ideal)
- **0-39**: Poor conditions (doesn't meet criteria)

#### 1.3 Prompt Structure

**System Prompts** (3 parts):

1. **Sport Evaluation Prompt**:
   - General guidelines for evaluating conditions for the sport
   - Example: "Evaluate wingfoiling conditions considering wind speed, gust consistency, and direction stability..."

2. **Spot-Specific Prompt**:
   - Characteristics of the specific spot
   - Example: "This spot works best with NW winds (315-45°), requires minimum 12 knots, and is protected from S winds..."

3. **Temporal Context Prompt**:
   - Instructions on how to use time series data
   - Example: "Consider trends in conditions 12 hours before and after. Improving conditions score higher than deteriorating conditions..."

**User Prompt** (condition data):

- Current slot data (wind, waves, tide)
- Time series context (72 hours before, 12 hours after)
- Spot metadata (name, location)

**Example Full Prompt**:

```
System: You are an expert watersports condition evaluator. Evaluate conditions for wingfoiling at Marina de Cascais. This spot works best with NW winds (315-45°), requires minimum 12 knots, and is protected from S winds. Consider trends in conditions 72 hours before and 12 hours after. Improving conditions score higher than deteriorating conditions.

User: Evaluate these conditions:
Current: 2024-01-15 14:00 - Wind: 18 knots, Gust: 22 knots, Direction: 330°, Waves: 0.5m
72h Before: 2024-01-12 14:00 - Wind: 8 knots, Gust: 10 knots, Direction: 270°
48h Before: 2024-01-13 14:00 - Wind: 12 knots, Gust: 15 knots, Direction: 300°
24h Before: 2024-01-14 14:00 - Wind: 15 knots, Gust: 18 knots, Direction: 320°
12h After: 2024-01-15 20:00 - Wind: 20 knots, Gust: 24 knots, Direction: 340°

Provide a score 0-100 with brief reasoning and factor breakdown.
```

### 2. Database Schema Changes

#### 2.1 Add Score Storage Table

**New Table**: `condition_scores`

**Rationale**: Separate table allows for user-specific scores in V2 (e.g., user prefers 15 knots vs system's 12 knots). Nested objects in `forecast_slots` would be problematic for this.

```typescript
condition_scores: defineTable({
  slotId: v.id("forecast_slots"), // Reference to forecast slot
  spotId: v.id("spots"), // Denormalized for efficient queries
  timestamp: v.number(), // Denormalized from slot (epoch ms)
  sport: v.string(), // Sport name (e.g., "wingfoil", "surfing")
  userId: v.optional(v.string()), // null = system/default score, user ID = personalized score
  score: v.number(), // 0-100
  reasoning: v.string(), // Brief explanation (1-2 sentences)
  factors: v.optional(
    v.object({
      windQuality: v.optional(v.number()),
      waveQuality: v.optional(v.number()),
      tideQuality: v.optional(v.number()),
      overallConditions: v.optional(v.number()),
    })
  ),
  scoredAt: v.number(), // Timestamp when scored (epoch ms)
  model: v.optional(v.string()), // LLM model used (e.g., "openai/gpt-oss-120b")
  scrapeTimestamp: v.optional(v.number()), // Denormalized for query efficiency
})
  .index("by_slot_sport", ["slotId", "sport"])
  .index("by_spot_timestamp_sport", ["spotId", "timestamp", "sport"])
  .index("by_user_spot_sport", ["userId", "spotId", "sport"]);
```

**For V1**: All scores have `userId: null` (system scores). V2 will add user-specific scores.

#### 2.2 Add Prompt Storage (V1)

**New Table**: `scoring_prompts`

**Rationale**: Store spot-specific prompts in database from V1. A spot can be suitable for multiple sports, so prompts are per spot-sport combination.

```typescript
scoring_prompts: defineTable({
  spotId: v.id("spots"),
  sport: v.string(), // e.g., "wingfoil", "surfing"
  userId: v.optional(v.string()), // null = default/system prompt, user ID = personalized prompt
  systemPrompt: v.string(), // Sport evaluation guidelines (general for the sport)
  spotPrompt: v.string(), // Spot-specific characteristics (e.g., "This spot works best with NW winds (315-45°), requires minimum 12 knots...")
  temporalPrompt: v.string(), // Temporal context instructions (e.g., "Consider trends in conditions 72 hours before and 12 hours after...")
  isActive: v.boolean(), // Enable/disable this prompt
  createdAt: v.number(),
  updatedAt: v.number(),
})
  .index("by_spot_sport", ["spotId", "sport"])
  .index("by_user_spot_sport", ["userId", "spotId", "sport"]);
```

**For V1**:

- All prompts have `userId: null` (system prompts)
- Load spot prompts from database when scoring (not hardcoded)
- Each spot-sport combination has its own prompt entry in database
- System prompts (sport evaluation guidelines) stored in `systemPrompt` field
- Spot-specific characteristics stored in `spotPrompt` field
- Temporal instructions stored in `temporalPrompt` field

**For V2**:

- Users can create personalized prompts (`userId` set)
- When scoring: Check for user prompt first, fall back to system prompt
- Lazy scoring for user-specific prompts (score on-demand when user views data)

### 3. Scoring Workflow

#### 3.1 Integration Point

**Scraping Flow** (Non-blocking):

```
saveForecastSlots mutation
  → Validate scrape data
  → Insert forecast slots (without scores)
  → Return success immediately
  → Trigger scoring process (async, non-blocking)
```

**Scoring Flow** (Separate Process):

```
scoreForecastSlots action (triggered after scrape)
  → For each slot in the scrape:
    → Fetch time series context (72h before, 12h after from DB)
    → For each sport this spot supports:
      → Get system prompt (sport evaluation guidelines)
      → Get spot-specific prompt from database (scoring_prompts table)
      → Call LLM scoring function (with retries, structured output)
      → Save score to condition_scores table via mutation
  → Log completion/failures
```

**Key Principles**:

- **Scraping never blocks on scoring**: Scrape completes and saves data immediately
- **Scoring is asynchronous**: Runs separately, can retry without affecting scrape
- **Scores added incrementally**: Slots may have scores added over time as scoring completes
- **Failure isolation**: Scoring failures don't affect scrape success

**Triggering Scoring**:

- **Option A**: Triggered immediately after scrape (fire-and-forget action) - **V1**
  - In `saveForecastSlots` mutation: After saving slots, call `scoreForecastSlots` action
  - Action runs asynchronously, doesn't block mutation return
  - Scraper script continues immediately
  - Scores system/default prompts only (userId: null)
- **Option B**: Background job that processes unscored slots periodically - **V2**
  - Useful for backfilling or retrying failed scores
  - Can pre-score for active users with custom prompts
- **Option C**: On-demand when slots are displayed (lazy scoring) - **V2 for user prompts**
  - Score slots when first accessed if not already scored
  - Best solution for personalized scores (don't waste resources on inactive users)
  - Cache scores once computed

**Decision**:

- **V1**: Use Option A for system scores (trigger after scrape, async action)
- **V2**: Add Option B for backfilling and Option C for user-specific lazy scoring

**Implementation Note**: The `saveForecastSlots` mutation will:

1. Save all slots to database
2. Return success immediately
3. Schedule `scoreForecastSlots` action (fire-and-forget via `ctx.scheduler` or direct action call)
4. Scoring happens in background, updates slots as it completes

#### 3.2 Time Series Context

**Data Source**: Existing `forecast_slots` in database

**Query Strategy**:

1. For each slot being scored:
   - Query slots for same spot within ±12 hours from database
   - Use most recent scrape's data (same `scrapeTimestamp` if available, otherwise latest)
   - Include newly scraped slots (already saved to DB) in the query

**Context Window**:

- **Before**: 72 hours (up to 72 slots if hourly data)
- **After**: 12 hours (up to 12 slots if hourly data)
- **Current**: The slot being scored

**Query Strategy**:

- Query slots for same spot within ±72 hours before and +12 hours after
- Use most recent scrape's data (same `scrapeTimestamp` if available, otherwise latest)
- Include newly scraped slots (already saved to DB) in the query
- Sort by timestamp for trend analysis

**Fallback**: If insufficient historical data (less than 24 hours before), use available data. Scoring can still proceed with limited context.

#### 3.3 Error Handling

**LLM API Failures**:

- **Retry Strategy**: 3 retries with exponential backoff delays (30s, 1 min, 5 min)
- **No Heuristic Fallback**: Completely remove heuristic-based scoring. If all retries fail:
  - Skip scoring for that slot (no score record created)
  - Log error for monitoring
  - Continue scoring other slots (don't block entire scoring process)
- **Logging**: Log all failures with full error details for debugging
- **Non-blocking**: Scoring failures never affect scraping - they are completely isolated

**Invalid Responses**:

- Validate score is 0-100 integer
- Validate reasoning is non-empty string
- Validate factors object structure (if present)
- If invalid: retry once with same backoff strategy (30s, 1 min, 5 min), then skip scoring

**Rate Limiting**:

- Groq rate limits: Monitor and handle 429 errors
- On 429: Implement exponential backoff with jitter (up to 30s delay)
- Implement request queuing if needed
- Batch requests if API supports it

### 4. Implementation Details

#### 4.1 New Convex Functions

**Action**: `spots.scoreForecastSlots`

```typescript
// External action (can call Groq API) - scores all slots from a scrape
export const scoreForecastSlots = action({
  args: {
    spotId: v.id("spots"),
    scrapeTimestamp: v.number(),
    slotIds: v.array(v.id("forecast_slots")), // Slots to score
  },
  handler: async (ctx, args) => {
    // For each slot:
    //   - Get time series context (query DB)
    //   - For each sport:
    //     - Call scoreSingleSlot action
    //   - Update slot with scores
    // Return summary (success count, failure count)
  },
});
```

**Action**: `spots.scoreSingleSlot`

```typescript
// Scores a single slot-sport combination
export const scoreSingleSlot = action({
  args: {
    slotId: v.id("forecast_slots"),
    sport: v.string(),
    spotId: v.id("spots"),
  },
  handler: async (ctx, args) => {
    // Get slot data
    // Get time series context (query DB)
    // Build prompt
    // Call Groq API (with retries)
    // Parse response
    // Update slot via mutation
    // Return score object
  },
});
```

**Mutation**: `spots.saveConditionScore`

```typescript
// Save a condition score to condition_scores table (called from action)
export const saveConditionScore = mutation({
  args: {
    slotId: v.id("forecast_slots"),
    spotId: v.id("spots"),
    timestamp: v.number(),
    sport: v.string(),
    userId: v.optional(v.string()), // null for system scores
    score: v.number(),
    reasoning: v.string(),
    factors: v.optional(
      v.object({
        windQuality: v.optional(v.number()),
        waveQuality: v.optional(v.number()),
        tideQuality: v.optional(v.number()),
        overallConditions: v.optional(v.number()),
      })
    ),
    model: v.optional(v.string()),
    scrapeTimestamp: v.optional(v.number()),
  },
  handler: async (ctx, args) => {
    // Insert or update score in condition_scores table
    // For system scores (userId: null), replace existing system score if present
    // For user scores, create new entry (multiple users can have scores for same slot-sport)
  },
});
```

**Query**: `spots.getTimeSeriesContext`

```typescript
// Query to get 72h before and 12h after slots from database
export const getTimeSeriesContext = query({
  args: {
    spotId: v.id("spots"),
    timestamp: v.number(),
    scrapeTimestamp: v.optional(v.number()),
  },
  handler: async (ctx, args) => {
    // Query slots within -72 hours to +12 hours of timestamp
    // Prefer same scrapeTimestamp, fallback to latest
    // Return sorted array by timestamp
  },
});
```

**Query**: `spots.getScoringPrompt`

```typescript
// Get scoring prompt for a spot-sport combination
export const getScoringPrompt = query({
  args: {
    spotId: v.id("spots"),
    sport: v.string(),
    userId: v.optional(v.string()), // null for system prompt
  },
  handler: async (ctx, args) => {
    // If userId provided: Check for user-specific prompt first
    // Fall back to system prompt (userId: null)
    // Return prompt object or null if not found
  },
});
```

#### 4.2 Groq API Integration

**Package**: `groq-sdk` (npm package)

**Configuration**:

- API Key: `GROQ_API_KEY` environment variable
- **Storage Location**:
  - **Convex**: If scoring runs in Convex actions, add to Convex environment variables
  - **Render**: If scoring runs on Render (e.g., separate worker service), add to Render environment variables
  - **Local Development**: `.env` or `.env.local`
- Model: `openai/gpt-oss-120b` (Groq-hosted OpenAI OSS model)
- Endpoint: `https://api.groq.com/openai/v1/chat/completions`

**Infrastructure Decision**:

- Scoring runs in Convex actions (serverless functions)
- Therefore, `GROQ_API_KEY` should be stored in Convex environment variables
- Convex actions can make external API calls to Groq

**Request Format**:

```typescript
{
  model: "openai/gpt-oss-120b",
  messages: [
    { role: "system", content: systemPrompt },
    { role: "user", content: userPrompt }
  ],
  temperature: 0.3,
  max_tokens: 500,
  response_format: {
    type: "json_object",
    schema: {
      type: "object",
      properties: {
        score: { type: "integer", minimum: 0, maximum: 100 },
        reasoning: { type: "string" },
        factors: {
          type: "object",
          properties: {
            windQuality: { type: "number", minimum: 0, maximum: 100 },
            waveQuality: { type: "number", minimum: 0, maximum: 100 },
            tideQuality: { type: "number", minimum: 0, maximum: 100 },
            overallConditions: { type: "number", minimum: 0, maximum: 100 }
          }
        }
      },
      required: ["score", "reasoning"]
    }
  }
}
```

**Response Parsing**:

- Parse JSON from `response.choices[0].message.content` (structured output ensures valid JSON)
- Extract `score` (0-100 integer), `reasoning` (string), and `factors` (optional object)
- Validate score is 0-100 integer
- Validate reasoning is non-empty string
- Validate factors structure if present

#### 4.3 Prompt Templates

**Location**: `convex/prompts.ts` (new file)

**Structure**:

```typescript
// System prompts (sport evaluation guidelines) - can be hardcoded or in DB
export const SYSTEM_SPORT_PROMPTS = {
  wingfoil:
    "Evaluate wingfoiling conditions considering wind speed, gust consistency, and direction stability...",
  surfing:
    "Evaluate surfing conditions considering wave height, period, direction, and tide conditions...",
};

// Spot-specific prompts loaded from database (scoring_prompts table)
// Temporal prompts also from database
// Function to build full prompt:
export function buildPrompt(
  systemPrompt,
  spotPrompt,
  temporalPrompt,
  slot,
  timeSeries
) {
  // Combine system prompt + spot prompt + temporal prompt
  // Format condition data with time series
  // Return full prompt object { system, user }
}
```

### 5. Migration Strategy

#### 5.0 Remove Heuristic-Based Scoring

**Complete Removal**:

- Remove all functions from `lib/criteria.js`:
  - `matchesWingfoilCriteria`
  - `matchesSurfingCriteria`
  - `isEpicConditions`
  - `findIdealSlot`
- Update `lib/slots.js`:
  - Remove `enrichSlots` logic that uses heuristic matching
  - Replace with score-based filtering (e.g., `slot.scores[sport]?.score >= threshold`)
- Update `app/page.js`:
  - Remove imports from `lib/criteria.js`
  - Update filtering logic to use LLM scores instead of `matchesCriteria`/`isIdeal` flags
- Remove or deprecate `spotConfigs` table usage for matching (keep for prompt generation)

**Timeline**: Remove heuristics as part of Phase 1 implementation, before deploying LLM scoring.

#### 5.1 Backfill Strategy

**V1**: No backfill of existing data

- Only new scrapes will be scored
- Existing forecast slots will have no scores
- Can backfill specific dates later for spot checks/testing

**Future**: Background scoring job for backfilling

- Can be implemented in V2 if needed
- Useful for testing prompts on historical data
- Can target specific date ranges for analysis

### 6. Performance Considerations

#### 6.1 Scoring Latency

**Target**: < 2 seconds per slot-sport combination

**Optimization Strategies**:

- Parallel scoring for multiple sports per slot
- Batch API requests if Groq supports
- Cache prompts (don't rebuild for each slot)
- Efficient time series queries (single query per slot)

#### 6.2 Cost Estimation

**Assumptions**:

- Average prompt: 500 tokens
- Average response: 200 tokens
- Cost: ~$0.10 per 1M input tokens, ~$0.60 per 1M output tokens (Groq pricing)

**Calculation**:

- Per slot-sport: ~700 tokens = ~$0.0002
- 100 slots × 2 sports = 200 scores = ~$0.04 per scrape
- Daily (3 scrapes): ~$0.12/day
- Monthly: ~$3.60/month

**Acceptable**: Yes, very low cost.

#### 6.3 Database Impact

**Storage**:

- Each score: ~200 bytes (score + reasoning)
- 100 slots × 2 sports = 40KB per scrape
- Negligible impact

**Query Performance**:

- Scores stored in same table (no joins needed)
- Index on `spotId` already exists
- No performance degradation expected

### 7. Monitoring & Observability

#### 7.1 Metrics to Track

- **Scoring Success Rate**: % of slots successfully scored
- **Average Scoring Latency**: Time per score
- **LLM API Errors**: Rate of failures, retries
- **Score Distribution**: Histogram of scores (0-100)
- **Cost Tracking**: Daily/monthly API costs

#### 7.2 Logging

- Log all LLM API calls (request/response)
- Log scoring failures with error details
- Log prompt templates used (for debugging)
- Log score updates to database

#### 7.3 Alerts

- Alert if scoring success rate < 95%
- Alert if average latency > 5 seconds
- Alert if API errors spike (> 10% failure rate)

### 8. V2: User Customization

#### 8.1 Custom Prompts

**Feature**: Allow users to define custom sport and sport-spot prompts

**Implementation**:

- Use `scoring_prompts` table (defined in section 2.2)
- UI for editing prompts (future PRD)
- When scoring: Check for user prompt first, fall back to default

**Example Use Cases**:

- "I prefer lighter wind conditions" (lower minSpeed in prompt)
- "I'm a beginner, score easier conditions higher"
- "This spot works better with S winds for me" (spot-specific)

#### 8.2 Personalization Features

- User-specific scoring preferences
- A/B testing different prompt strategies
- Learning from user feedback (future)

---

## Technical Implementation Plan

### Phase 1: Core LLM Scoring (V1)

1. **Setup Groq Integration**
   - Add `groq-sdk` package
   - Configure `GROQ_API_KEY` in Convex environment variables (since scoring runs in Convex actions)
   - Use model `openai/gpt-oss-120b`
   - Enable structured outputs in API calls
   - Create test script

2. **Database Schema Updates**
   - Add `scores` field to `forecast_slots` schema
   - Create migration if needed

3. **Prompt System**
   - Create `scoring_prompts` table in schema
   - Seed database with system prompts for each spot-sport combination
   - Create `convex/prompts.ts` helper functions
   - Load prompts from database (not hardcoded)
   - System sport prompts can be hardcoded or in DB (TBD)

4. **Scoring Functions**
   - Create `spots.scoreForecastSlots` action (scores all slots from scrape)
   - Create `spots.scoreSingleSlot` action (scores one slot-sport)
   - Create `spots.getTimeSeriesContext` query (72h before, 12h after)
   - Create `spots.getScoringPrompt` query (loads from DB)
   - Create `spots.saveConditionScore` mutation (saves to condition_scores table)

5. **Integration**
   - Keep `saveForecastSlots` simple (no scoring)
   - Trigger `scoreForecastSlots` action after scrape completes (fire-and-forget)
   - Action loads prompts from database
   - Action uses structured outputs for LLM calls
   - Add error handling and retries (30s, 1 min, 5 min) in scoring action
   - Add logging for both scrape and scoring processes

6. **Database Schema Updates**
   - Add `condition_scores` table
   - Add `scoring_prompts` table
   - Add indexes for efficient queries

7. **Testing**
   - Unit tests for prompt building
   - Integration tests for scoring flow
   - Test structured outputs with Groq API
   - Test time series context queries (72h before, 12h after)

### Phase 2: Optimization & Monitoring

1. **Performance Optimization**
   - Parallel scoring
   - Prompt caching
   - Batch requests if possible

2. **Monitoring**
   - Add metrics tracking
   - Set up alerts
   - Cost tracking

3. **Error Handling Improvements**
   - Enhanced retry logic with jitter
   - Better error categorization
   - Comprehensive logging and monitoring

### Phase 3: User Customization (V2)

1. **Database Schema**
   - Create `scoring_prompts` table
   - Add indexes

2. **API**
   - CRUD operations for prompts
   - Prompt selection logic

3. **UI** (separate PRD)
   - Prompt editor
   - Prompt management
   - Preview/test prompts

---

## Open Questions

1. **Score Format**: Should we store just a number (0-100) or include reasoning/factors?
   - **Decision**: Include reasoning for transparency, factors optional

2. **Time Series Source**: Use existing DB data or only current scrape?
   - **Decision**: Use existing DB data (more accurate trends)

3. **Scoring Timing**: Synchronous during save or async background?
   - **Decision**: Async background process (fire-and-forget action). Scraping never blocks on scoring.

4. **Fallback Strategy**: What if LLM fails?
   - **Decision**: No heuristic fallback. Skip scoring (null), log error, continue with scrape. Retry with exponential backoff (1s, 2s, 4s) up to 3 attempts.

5. **Model Selection**: Which Groq model?
   - **Decision**: `openai/gpt-oss-120b` (Groq-hosted OpenAI OSS model)

6. **Prompt Storage**: Hardcoded or database?
   - **Decision**: Hardcoded for V1, database for V2

---

## Success Criteria

### V1 Success Metrics

- ✅ Scraping completes without blocking on scoring
- ✅ Scoring triggered automatically after scrape (async)
- ✅ Scoring success rate > 95%
- ✅ Average scoring latency < 3 seconds per slot-sport
- ✅ Scores stored correctly in database
- ✅ No degradation in scrape performance (< 30s total per spot, scoring happens separately)

### V2 Success Metrics

- ✅ Users can create custom prompts
- ✅ Custom prompts used for scoring
- ✅ Prompt management UI functional
- ✅ No performance regression

---

## Risks & Mitigations

### Risk 1: LLM API Unavailability

- **Impact**: Low - Scoring fails for affected slots, but scraping is completely unaffected (non-blocking)
- **Mitigation**: Retry with exponential backoff (1s, 2s, 4s), skip scoring on final failure. No heuristic fallback - scores will be null for failed slots. Can retry scoring later without re-scraping.

### Risk 2: High Latency

- **Impact**: Medium - Slow scrapes
- **Mitigation**: Parallel scoring, async processing, faster model

### Risk 3: Cost Overruns

- **Impact**: Low - Estimated cost is very low
- **Mitigation**: Monitor costs, set budget alerts, optimize prompts

### Risk 4: Inconsistent Scores

- **Impact**: Medium - User confusion
- **Mitigation**: Low temperature (0.3), validate responses, test prompts

### Risk 5: Prompt Engineering Complexity

- **Impact**: Medium - Requires iteration
- **Mitigation**: Start simple, iterate based on results, document learnings

---

## Dependencies

- Groq API account and API key
- `groq-sdk` npm package
- Convex actions support (for external API calls)
- Existing `forecast_slots` data structure

---

## References

- [Groq API Documentation](https://console.groq.com/docs)
- [Convex Actions Documentation](https://docs.convex.dev/functions/actions)
- Current scoring logic: `lib/criteria.js`
- Current scraping flow: `scripts/scrape.mjs`, `convex/spots.ts`

---

**Document Maintained By**: Engineering Team  
**Last Updated**: 2025-12-28
